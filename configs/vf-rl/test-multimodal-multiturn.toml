# Test configuration for multi-turn multimodal training
# Uses a simple dummy environment with colored images and multiple turns

model = "Qwen/Qwen3-VL-8B-Instruct"

[env]
# Use the multi-turn test environment (loaded via python path)
id = "test_env_multimodal_multiturn"  # Python module name

[env.args]
num_examples = 1  # Single example for observation
max_turns = 3  # Max 3 guesses per image

[inference]
gpus = 1

[inference.args]
enforce_eager = true
max_model_len = 2048

[trainer]
gpus = 1

[trainer.args]
run_name = "test-multimodal-multiturn-qwen3vl"
output_dir = "./outputs/test-multimodal-multiturn"

# Small values for quick testing
learning_rate = 1e-5
max_steps = 3  # Just 3 steps to test
batch_size = 1
micro_batch_size = 1  # Single batch for observation
rollouts_per_step = 1  # Single rollout for observation

# GRPO clipping
mask_ratio_low = 0.1
mask_ratio_high = 10.0
temperature = 1.0

# vLLM config
vllm_server_host = "0.0.0.0"
vllm_server_port = 8000

# Rollout config
max_concurrent = 1
generation_timeout = 300.0

# Weight syncing - disabled for testing
sync_to_vllm_every = 999999  # Effectively disabled

# Logging
log_every_steps = 1

